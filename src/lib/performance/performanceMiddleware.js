// Middleware de performance pour optimisation automatique
import { NextResponse } from 'next/server'
import { memoryCache } from './cache'

/**
 * Classe pour mesurer les performances des API
 */
class PerformanceMonitor {
  constructor() {
    this.metrics = new Map()
    this.slowQueries = []
    this.startTime = Date.now()
  }

  recordMetric(endpoint, method, duration, statusCode, cacheHit = false) {
    const key = `${method}_${endpoint}`
    const metric = this.metrics.get(key) || {
      count: 0,
      totalDuration: 0,
      averageDuration: 0,
      minDuration: Infinity,
      maxDuration: 0,
      errorCount: 0,
      cacheHits: 0,
      lastAccessed: null
    }

    metric.count++
    metric.totalDuration += duration
    metric.averageDuration = metric.totalDuration / metric.count
    metric.minDuration = Math.min(metric.minDuration, duration)
    metric.maxDuration = Math.max(metric.maxDuration, duration)
    metric.lastAccessed = new Date().toISOString()

    if (statusCode >= 400) {
      metric.errorCount++
    }

    if (cacheHit) {
      metric.cacheHits++
    }

    // Enregistrer les requÃªtes lentes
    if (duration > 1000) {
      this.slowQueries.push({
        endpoint: key,
        duration,
        timestamp: new Date().toISOString(),
        statusCode
      })

      // Garder seulement les 50 derniÃ¨res requÃªtes lentes
      if (this.slowQueries.length > 50) {
        this.slowQueries = this.slowQueries.slice(-50)
      }
    }

    this.metrics.set(key, metric)
  }

  getMetrics() {
    const allMetrics = Array.from(this.metrics.entries()).map(([endpoint, data]) => ({
      endpoint,
      ...data,
      errorRate: data.count > 0 ? (data.errorCount / data.count * 100).toFixed(2) + '%' : '0%',
      cacheHitRate: data.count > 0 ? (data.cacheHits / data.count * 100).toFixed(2) + '%' : '0%'
    }))

    return {
      uptime: Date.now() - this.startTime,
      totalRequests: allMetrics.reduce((sum, m) => sum + m.count, 0),
      averageResponseTime: allMetrics.reduce((sum, m) => sum + m.averageDuration, 0) / allMetrics.length || 0,
      slowQueries: this.slowQueries,
      topEndpoints: allMetrics.sort((a, b) => b.count - a.count).slice(0, 10),
      slowestEndpoints: allMetrics.sort((a, b) => b.averageDuration - a.averageDuration).slice(0, 10)
    }
  }

  reset() {
    this.metrics.clear()
    this.slowQueries = []
    this.startTime = Date.now()
  }
}

// Instance globale du moniteur
export const performanceMonitor = new PerformanceMonitor()

/**
 * Middleware de performance principal
 */
export function withPerformanceOptimization(handler, options = {}) {
  const {
    enableCache = true,
    cacheTTL = 300000, // 5 minutes
    enableCompression = true,
    enableMetrics = true,
    slowQueryThreshold = 1000
  } = options

  return async (request) => {
    const startTime = Date.now()
    const method = request.method
    const url = new URL(request.url)
    const endpoint = url.pathname

    // Headers de performance
    const performanceHeaders = new Headers()
    performanceHeaders.set('X-Response-Time-Start', startTime.toString())

    try {
      // VÃ©rifier le cache pour les requÃªtes GET
      if (enableCache && method === 'GET') {
        const cacheKey = `api_${endpoint}_${url.search}`
        const cached = memoryCache.get(cacheKey)
        
        if (cached) {
          const duration = Date.now() - startTime
          
          if (enableMetrics) {
            performanceMonitor.recordMetric(endpoint, method, duration, 200, true)
          }

          const response = NextResponse.json(cached)
          response.headers.set('X-Cache', 'HIT')
          response.headers.set('X-Response-Time', `${duration}ms`)
          
          return response
        }
      }

      // ExÃ©cuter le handler original
      const response = await handler(request)
      const duration = Date.now() - startTime

      // Enregistrer les mÃ©triques
      if (enableMetrics) {
        performanceMonitor.recordMetric(endpoint, method, duration, response.status, false)
      }

      // Ajouter headers de performance
      response.headers.set('X-Response-Time', `${duration}ms`)
      response.headers.set('X-Cache', 'MISS')

      // Warning pour les requÃªtes lentes
      if (duration > slowQueryThreshold) {
        console.warn(`ðŸŒ RequÃªte lente dÃ©tectÃ©e: ${method} ${endpoint} - ${duration}ms`)
        response.headers.set('X-Slow-Query', 'true')
      }

      // Mettre en cache si c'est un GET avec succÃ¨s
      if (enableCache && method === 'GET' && response.ok) {
        try {
          const responseClone = response.clone()
          const data = await responseClone.json()
          const cacheKey = `api_${endpoint}_${url.search}`
          memoryCache.set(cacheKey, data, cacheTTL)
        } catch (error) {
          console.warn('Impossible de mettre en cache la rÃ©ponse:', error)
        }
      }

      // Compression (simulÃ©e avec header)
      if (enableCompression) {
        response.headers.set('X-Compression', 'enabled')
      }

      return response

    } catch (error) {
      const duration = Date.now() - startTime
      
      if (enableMetrics) {
        performanceMonitor.recordMetric(endpoint, method, duration, 500, false)
      }

      console.error(`Erreur performance ${method} ${endpoint}:`, error)
      
      const errorResponse = NextResponse.json(
        { error: 'Erreur interne du serveur' },
        { status: 500 }
      )
      
      errorResponse.headers.set('X-Response-Time', `${duration}ms`)
      
      return errorResponse
    }
  }
}

/**
 * Analyseur de performance automatique
 */
export class PerformanceAnalyzer {
  constructor() {
    this.analysisInterval = null
    this.recommendations = []
  }

  startAnalysis(intervalMs = 60000) { // Analyse toutes les minutes
    this.analysisInterval = setInterval(() => {
      this.analyze()
    }, intervalMs)
  }

  stopAnalysis() {
    if (this.analysisInterval) {
      clearInterval(this.analysisInterval)
      this.analysisInterval = null
    }
  }

  analyze() {
    const metrics = performanceMonitor.getMetrics()
    this.recommendations = []

    // Analyser les endpoints lents
    const slowEndpoints = metrics.slowestEndpoints.filter(e => e.averageDuration > 500)
    if (slowEndpoints.length > 0) {
      this.recommendations.push({
        type: 'performance',
        severity: 'high',
        message: `${slowEndpoints.length} endpoints lents dÃ©tectÃ©s`,
        details: slowEndpoints.map(e => `${e.endpoint}: ${e.averageDuration.toFixed(0)}ms`),
        suggestion: 'Optimiser les requÃªtes SQL, ajouter des index, ou implÃ©menter du cache'
      })
    }

    // Analyser les taux d'erreur Ã©levÃ©s
    const highErrorEndpoints = metrics.topEndpoints.filter(e => 
      parseFloat(e.errorRate) > 5
    )
    if (highErrorEndpoints.length > 0) {
      this.recommendations.push({
        type: 'reliability',
        severity: 'medium',
        message: `${highErrorEndpoints.length} endpoints avec taux d'erreur Ã©levÃ©`,
        details: highErrorEndpoints.map(e => `${e.endpoint}: ${e.errorRate}`),
        suggestion: 'VÃ©rifier la validation des donnÃ©es et la gestion d\'erreurs'
      })
    }

    // Analyser l'efficacitÃ© du cache
    const lowCacheEndpoints = metrics.topEndpoints.filter(e => 
      parseFloat(e.cacheHitRate) < 30 && e.count > 10
    )
    if (lowCacheEndpoints.length > 0) {
      this.recommendations.push({
        type: 'cache',
        severity: 'low',
        message: `${lowCacheEndpoints.length} endpoints avec faible taux de cache`,
        details: lowCacheEndpoints.map(e => `${e.endpoint}: ${e.cacheHitRate}`),
        suggestion: 'Augmenter la durÃ©e de cache ou amÃ©liorer la stratÃ©gie de cache'
      })
    }

    // Log des recommandations importantes
    const highSeverity = this.recommendations.filter(r => r.severity === 'high')
    if (highSeverity.length > 0) {
      console.warn('ðŸ” Recommandations de performance importantes:', highSeverity)
    }
  }

  getRecommendations() {
    return this.recommendations
  }
}

// Instance globale de l'analyseur
export const performanceAnalyzer = new PerformanceAnalyzer()

/**
 * Optimiseur automatique de cache
 */
export class CacheOptimizer {
  constructor() {
    this.optimizationRules = new Map()
  }

  addRule(pattern, ttl, condition = null) {
    this.optimizationRules.set(pattern, { ttl, condition })
  }

  getOptimalTTL(endpoint, responseData) {
    // TTL par dÃ©faut basÃ© sur le type d'endpoint
    let baseTTL = 300000 // 5 minutes

    // RÃ¨gles prÃ©dÃ©finies
    if (endpoint.includes('/services')) {
      baseTTL = 600000 // 10 minutes pour les services
    } else if (endpoint.includes('/stats')) {
      baseTTL = 180000 // 3 minutes pour les stats
    } else if (endpoint.includes('/orders')) {
      baseTTL = 60000 // 1 minute pour les commandes
    }

    // Appliquer les rÃ¨gles personnalisÃ©es
    for (const [pattern, rule] of this.optimizationRules) {
      if (endpoint.includes(pattern)) {
        if (!rule.condition || rule.condition(responseData)) {
          baseTTL = rule.ttl
          break
        }
      }
    }

    return baseTTL
  }

  shouldCache(endpoint, method, responseSize) {
    // Ne pas cacher les endpoints de modification
    if (['POST', 'PUT', 'DELETE', 'PATCH'].includes(method)) {
      return false
    }

    // Ne pas cacher les rÃ©ponses trop volumineuses
    if (responseSize > 1024 * 1024) { // 1MB
      return false
    }

    // Ne pas cacher les endpoints d'authentification
    if (endpoint.includes('/auth') || endpoint.includes('/login')) {
      return false
    }

    return true
  }
}

// Instance globale de l'optimiseur
export const cacheOptimizer = new CacheOptimizer()

/**
 * Utilitaires de performance
 */
export const performanceUtils = {
  // Mesurer le temps d'exÃ©cution d'une fonction
  async measureAsync(fn, label = 'Operation') {
    const start = performance.now()
    const result = await fn()
    const duration = performance.now() - start
    
    console.log(`â±ï¸ ${label}: ${duration.toFixed(2)}ms`)
    
    return { result, duration }
  },

  // CrÃ©er un debouncer
  debounce(fn, delay) {
    let timeoutId
    return (...args) => {
      clearTimeout(timeoutId)
      timeoutId = setTimeout(() => fn.apply(this, args), delay)
    }
  },

  // CrÃ©er un throttler
  throttle(fn, delay) {
    let lastTime = 0
    return (...args) => {
      const now = Date.now()
      if (now - lastTime >= delay) {
        lastTime = now
        return fn.apply(this, args)
      }
    }
  },

  // Batching de requÃªtes
  createBatcher(batchFn, { maxBatchSize = 10, maxWaitTime = 100 } = {}) {
    let batch = []
    let timer = null

    const processBatch = async () => {
      if (batch.length === 0) return
      
      const currentBatch = batch.splice(0, maxBatchSize)
      try {
        await batchFn(currentBatch)
      } catch (error) {
        console.error('Erreur batch:', error)
      }
    }

    return (item) => {
      batch.push(item)

      // Traiter immÃ©diatement si le batch est plein
      if (batch.length >= maxBatchSize) {
        if (timer) clearTimeout(timer)
        processBatch()
        return
      }

      // Programmer le traitement
      if (timer) clearTimeout(timer)
      timer = setTimeout(processBatch, maxWaitTime)
    }
  }
}

/**
 * Hook React pour monitoring des performances cÃ´tÃ© client
 */
export function usePerformanceMonitoring(componentName) {
  useEffect(() => {
    const startTime = performance.now()
    
    return () => {
      const endTime = performance.now()
      const renderTime = endTime - startTime
      
      // Log des composants lents
      if (renderTime > 16) {
        console.warn(`ðŸŒ Composant lent: ${componentName} - ${renderTime.toFixed(2)}ms`)
      }

      // Envoyer les mÃ©triques (en production, envoyer Ã  un service d'analytics)
      if (typeof window !== 'undefined' && window.gtag) {
        window.gtag('event', 'component_performance', {
          component_name: componentName,
          render_time: renderTime
        })
      }
    }
  }, [componentName])
}

/**
 * Initialisation du systÃ¨me de performance
 */
export function initializePerformanceSystem() {
  // DÃ©marrer l'analyse automatique
  performanceAnalyzer.startAnalysis()

  // Configurer les rÃ¨gles d'optimisation du cache
  cacheOptimizer.addRule('/services', 600000) // 10 minutes
  cacheOptimizer.addRule('/platforms', 900000) // 15 minutes
  cacheOptimizer.addRule('/stats', 180000) // 3 minutes

  console.log('ðŸš€ SystÃ¨me de performance initialisÃ©')
}

/**
 * API endpoint pour les mÃ©triques de performance
 */
export function createPerformanceAPI() {
  return {
    getMetrics: () => performanceMonitor.getMetrics(),
    getRecommendations: () => performanceAnalyzer.getRecommendations(),
    getCacheStats: () => memoryCache.getStats(),
    reset: () => {
      performanceMonitor.reset()
      memoryCache.clear()
    }
  }
}
